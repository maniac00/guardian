{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bae833fe-518d-4c5d-9e60-45dedac70c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import os\n",
    "import random  \n",
    "import gc\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import Image, clear_output\n",
    "from torch.cuda import memory_allocated, empty_cache\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from glob import glob\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2163364-85c1-4a65-8fa9-7b3859c717fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/kimsungwook/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 🚀 2024-4-17 Python-3.11.7 torch-2.2.2 CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "# torch_ver Yolov5\n",
    "yolo_model = torch.hub.load('ultralytics/yolov5', 'yolov5s',\n",
    "                            device='cuda:0' if torch.cuda.is_available() else 'cpu')  # 예측 모델\n",
    "yolo_model.classes = [0]  # 예측 클래스 (0 : 사람)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a28a20db-09d0-4a17-a818-ae3ba02bd6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM\n",
    "class skeleton_LSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(skeleton_LSTM, self).__init__()\n",
    "        self.lstm1 = nn.LSTM(input_size=len(attention_dot) * 2, hidden_size=128, num_layers=NUM_LAYERS, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(input_size=128, hidden_size=256, num_layers=NUM_LAYERS, batch_first=True)\n",
    "        self.lstm3 = nn.LSTM(input_size=256, hidden_size=512, num_layers=NUM_LAYERS, batch_first=True)\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        self.lstm4 = nn.LSTM(input_size=512, hidden_size=256, num_layers=NUM_LAYERS, batch_first=True)\n",
    "        self.lstm5 = nn.LSTM(input_size=256, hidden_size=128, num_layers=NUM_LAYERS, batch_first=True)\n",
    "        self.lstm6 = nn.LSTM(input_size=128, hidden_size=64, num_layers=NUM_LAYERS, batch_first=True)\n",
    "        self.dropout2 = nn.Dropout(0.1)\n",
    "        self.lstm7 = nn.LSTM(input_size=64, hidden_size=32, num_layers=NUM_LAYERS, batch_first=True)\n",
    "        self.fc = nn.Linear(32,2)\n",
    "\n",
    "    def forward(self, x) :\n",
    "        x, _ = self.lstm1(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "        x, _ = self.lstm3(x)\n",
    "        x = self.dropout1(x)\n",
    "        x, _ = self.lstm4(x)\n",
    "        x, _ = self.lstm5(x)\n",
    "        x, _ = self.lstm6(x)\n",
    "        x = self.dropout2(x)\n",
    "        x, _ = self.lstm7(x)\n",
    "        x = self.fc(x[:,-1,:]) # x[배치 크기, 시퀀스 길이, 은닉 상태 크기], [:, -1, :] -> 마지막 시간 단계만 선택\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74ef6941-86c9-4e85-9c49-7fb4f8f2809c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pose : Only Body\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 6\n",
    "EPOCH = 300\n",
    "NUM_LAYERS = 3      # LSTM model: num_layers\n",
    "start_dot = 11      # mp.solutions.pose 시작 포인트 (0: 얼굴부터 발목까지, 11: 어깨부터 발목까지)\n",
    "n_CONFIDENCE = 0.3    # MediaPipe Min Detectin confidence check\n",
    "y_CONFIDENCE = 0.3    # Yolv5 Min Detectin confidence check\n",
    "\n",
    "mp_pose = mp.solutions.pose\n",
    "attention_dot = [n for n in range(start_dot, 29)]\n",
    "\n",
    "# 라인 그리기\n",
    "if start_dot == 11:\n",
    "    \"\"\"몸 부분만\"\"\"\n",
    "    draw_line = [[11, 13], [13, 15], [15, 21], [15, 19], [15, 17], [17, 19], \\\n",
    "                [12, 14], [14, 16], [16, 22], [16, 20], [16, 18], [18, 20], \\\n",
    "                [23, 25], [25, 27], [24, 26], [26, 28], [11, 12], [11, 23], \\\n",
    "                [23, 24], [12, 24]]\n",
    "    print('Pose : Only Body')\n",
    "\n",
    "else:\n",
    "    \"\"\"얼굴 포함\"\"\"\n",
    "    draw_line = [[11, 13], [13, 15], [15, 21], [15, 19], [15, 17], [17, 19], \\\n",
    "                [12, 14], [14, 16], [16, 22], [16, 20], [16, 18], [18, 20], \\\n",
    "                [23, 25], [25, 27], [24, 26], [26, 28], [11, 12], [11, 23], \\\n",
    "                [23, 24], [12, 24], [9, 10], [0, 5], [0, 2], [5, 8], [2, 7]]\n",
    "    print('Pose : Face + Body')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31254c0e-5ecf-4de6-bc1a-260ccec34d9d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = skeleton_LSTM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a4f8a16-db5b-45cb-bb11-e7d8c7fcaa15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('LSTM1.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "895e393a-3514-418c-b20f-229b55235b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yolov4 바운딩 box 안에서 media pipe 데이터 전처리 함수\n",
    "\n",
    "def get_skeleton(video_path, attention_dot, draw_line):\n",
    "    frame_length = 30 # LSTM 모델에 넣을 frame 수\n",
    "\n",
    "    xy_list_list, xy_list_list_flip = [], []\n",
    "    cv2.destroyAllWindows()\n",
    "    pose = mp_pose.Pose(static_image_mode = True, model_complexity = 1, \\\n",
    "                        enable_segmentation = False, min_detection_confidence = n_CONFIDENCE)\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    if cap.isOpened():\n",
    "\n",
    "        while True:\n",
    "            ret, img = cap.read()\n",
    "\n",
    "            if ret == True:\n",
    "\n",
    "                \"\"\" Yolo 바운딩 박스 및 좌표 추출\"\"\"\n",
    "                img = cv2.resize(img, (640, 640))\n",
    "                res = yolo_model(img)\n",
    "                res_refine = res.pandas().xyxy[0].values\n",
    "                nms_human = len(res_refine)\n",
    "                if nms_human > 0:\n",
    "                    for bbox in res_refine:\n",
    "                        \"\"\"바운딩 박스 상하좌우 크기 조절\"\"\"\n",
    "                        xx1, yy1, xx2, yy2 = int(bbox[0])-10, int(bbox[1]), int(bbox[2])+10, int(bbox[3])\n",
    "                        if xx1 < 0:\n",
    "                            xx1 = 0\n",
    "                        elif xx2 > 639:\n",
    "                            xx2 = 639\n",
    "                        if yy1 < 0:\n",
    "                            yy1 = 0\n",
    "                        elif yy2 > 639:\n",
    "                            yy2 = 639\n",
    "\n",
    "                        start_point = (xx1, yy1)\n",
    "                        end_point = (xx2, yy2)\n",
    "\n",
    "                        \"\"\" Yolov5 바운딩 박스 좌표 안에서 mediapipe Pose 추출\"\"\"\n",
    "                        if bbox[4] > y_CONFIDENCE: # bbox[4] : confidence 데이터\n",
    "                            # img = cv2.rectangle(img, start_point, end_point, (0, 0, 255), 2) # 바운딩 박스 그리기 : 데이터 추출 확인용\n",
    "                            c_img = img[yy1:yy2, xx1:xx2] # 바운딩 박스 좌표\n",
    "                            results = pose.process(cv2.cvtColor(c_img, cv2.COLOR_BGR2RGB)) # Yolov5 바운딩 박스 좌표 안에서 'mp_pose' 좌표\n",
    "\n",
    "                            if not results.pose_landmarks: continue\n",
    "                            idx = 0\n",
    "                            draw_line_dic = {}\n",
    "                            xy_list, xy_list_flip = [], []\n",
    "                            # 33 반복문 진행 : 33개 중 18개의 dot\n",
    "                            for x_and_y in results.pose_landmarks.landmark:\n",
    "                                if idx in attention_dot:\n",
    "                                    xy_list.append(x_and_y.x)\n",
    "                                    xy_list.append(x_and_y.y)\n",
    "                                    xy_list_flip.append(1 - x_and_y.x)\n",
    "                                    xy_list_flip.append(x_and_y.y)\n",
    "\n",
    "                                    x, y = int(x_and_y.x*(xx2-xx1)), int(x_and_y.y*(yy2-yy1))\n",
    "                                    draw_line_dic[idx] = [x, y]\n",
    "                                idx += 1\n",
    "\n",
    "                            if len(xy_list) != len(attention_dot) * 2:\n",
    "                                print('Error : attention_dot 데이터 오류')\n",
    "\n",
    "                            xy_list_list.append(xy_list)\n",
    "                            xy_list_list_flip.append(xy_list_flip)\n",
    "\n",
    "                            \"\"\"mediapipe line 그리기 부분 : 데이터 추출(dot) 확인용\"\"\"\n",
    "                            # for line in draw_line:\n",
    "                            #     x1, y1 = draw_line_dic[line[0]][0], draw_line_dic[line[0]][1]\n",
    "                            #     x2, y2 = draw_line_dic[line[1]][0], draw_line_dic[line[1]][1]\n",
    "                            #     c_img = cv2.line(c_img, (x1, y1), (x2, y2), (0, 255, 0), 4)\n",
    "                            # # cv2.imshow('Landmark Image', img)\n",
    "                            # cv2_imshow(img)\n",
    "                            # cv2.waitKey(1)\n",
    "\n",
    "            elif ret == False: break\n",
    "\n",
    "\n",
    "        # 부족한 프레임 수 맞추기\n",
    "        if len(xy_list_list_flip) < 15:\n",
    "            return False, False\n",
    "        elif len(xy_list_list_flip) < frame_length:\n",
    "            f_ln = frame_length - len(xy_list_list_flip)\n",
    "            for _ in range(f_ln):\n",
    "                xy_list_list.append(xy_list_list[-1])\n",
    "                xy_list_list_flip.append(xy_list_list_flip[-1])\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "    return xy_list_list, xy_list_list_flip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54d950f5-caaa-495a-ac7d-df31d500b043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "저장된 frame의 개수: 150\n"
     ]
    }
   ],
   "source": [
    "# 영상 resize 및 추출\n",
    "#test_video_name = 'C_3_12_43_BU_SMC_10-14_12-17-14_CC_RGB_DF2_F2'\n",
    "test_video_path = f'01_pocket_30.mp4'\n",
    "cv2.destroyAllWindows()\n",
    "cap = cv2.VideoCapture(test_video_path)\n",
    "img_list = []\n",
    "\n",
    "if cap.isOpened():\n",
    "\n",
    "    while True:\n",
    "        ret, img = cap.read()\n",
    "        if ret:\n",
    "            img = cv2.resize(img, (640, 640))\n",
    "            img_list.append(img)\n",
    "            # cv2_imshow(img)\n",
    "            # cv2.waitKey(1)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print('저장된 frame의 개수: {}'.format(len(img_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61fde9f9-6857-4f15-a3cd-fb5e5bbaa30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, seq_list):\n",
    "        self.X = []\n",
    "        self.y = []\n",
    "        for dic in seq_list :\n",
    "            self.y.append(dic['key'])\n",
    "            self.X.append(dic['value'])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = self.X[index]\n",
    "        label = self.y[index]\n",
    "        return torch.Tensor(np.array(data)), torch.tensor(np.array(int(label)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd51a595-6bda-4395-8b16-b19889748773",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43474f93-f0ac-44f4-ac68-4d59750051a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1713832264.506778 1408120 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "시퀀스 데이터 분석 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████| 150/150 [00:17<00:00,  8.67it/s]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Yolov5 + Mediapipe Version\"\"\"\n",
    "\n",
    "model.eval()\n",
    "\n",
    "length = 30 # frame 상태를 표시할 길이\n",
    "out_img_list = []\n",
    "dataset = []\n",
    "status = 'None'\n",
    "pose = mp_pose.Pose(static_image_mode=True, model_complexity=1, enable_segmentation=False, min_detection_confidence=n_CONFIDENCE)\n",
    "print('시퀀스 데이터 분석 중...')\n",
    "\n",
    "xy_list_list = []\n",
    "for img in tqdm(img_list):\n",
    "    res = yolo_model(img)\n",
    "    res_refine = res.pandas().xyxy[0].values\n",
    "\n",
    "    nms_human = len(res_refine)\n",
    "    if nms_human > 0:\n",
    "        for bbox in res_refine:\n",
    "            xx1, yy1, xx2, yy2 = int(bbox[0])-10, int(bbox[1]), int(bbox[2])+10, int(bbox[3])\n",
    "            if xx1 < 0:\n",
    "                xx1 = 0\n",
    "            elif xx2 > 639:\n",
    "                xx2 = 639\n",
    "            if yy1 < 0:\n",
    "                yy1 = 0\n",
    "            elif yy2 > 639:\n",
    "                yy2 = 639\n",
    "\n",
    "            start_point = (xx1, yy1)\n",
    "            end_point = (xx2, yy2)\n",
    "            if bbox[4] > y_CONFIDENCE:\n",
    "                img = cv2.rectangle(img, start_point, end_point, (0, 0, 255), 2)\n",
    "\n",
    "                c_img = img[yy1:yy2, xx1:xx2]\n",
    "                results = pose.process(cv2.cvtColor(c_img, cv2.COLOR_BGR2RGB)) # Yolo 바운딩 box 안에서 landmark dot 추출\n",
    "                if not results.pose_landmarks: continue\n",
    "                xy_list = []\n",
    "                idx = 0\n",
    "                draw_line_dic = {}\n",
    "                for x_and_y in results.pose_landmarks.landmark:\n",
    "                    if idx in attention_dot:\n",
    "                        xy_list.append(x_and_y.x)\n",
    "                        xy_list.append(x_and_y.y)\n",
    "                        x, y = int(x_and_y.x*(xx2-xx1)), int(x_and_y.y*(yy2-yy1))\n",
    "                        draw_line_dic[idx] = [x, y]\n",
    "                    idx += 1\n",
    "\n",
    "                xy_list_list.append(xy_list)\n",
    "                for line in draw_line:\n",
    "                    x1, y1 = draw_line_dic[line[0]][0], draw_line_dic[line[0]][1]\n",
    "                    x2, y2 = draw_line_dic[line[1]][0], draw_line_dic[line[1]][1]\n",
    "                    c_img = cv2.line(c_img, (x1, y1), (x2, y2), (0, 255, 0), 3)\n",
    "\n",
    "                if len(xy_list_list) == length:\n",
    "                    dataset = []\n",
    "                    dataset.append({'key' : 0, 'value' : xy_list_list})\n",
    "                    dataset = MyDataset(dataset)\n",
    "                    dataset = DataLoader(dataset)\n",
    "                    xy_list_list = []\n",
    "\n",
    "                    for data, label in dataset:\n",
    "                        data = data.to(device)\n",
    "                        with torch.no_grad():\n",
    "                            result = model(data)\n",
    "                            _, out = torch.max(result, 1)\n",
    "                            if out.item() == 0: status = 'Normal'\n",
    "                            else: status = 'Theft'\n",
    "\n",
    "    cv2.putText(img, status, (0, 50), cv2.FONT_HERSHEY_COMPLEX, 1.5, (0, 0, 255), 2)\n",
    "    out_img_list.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ed2f6f7-dbec-4205-944b-5cdc9e44f213",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: FFMPEG: tag 0x58564944/'DIVX' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n"
     ]
    }
   ],
   "source": [
    "# 테스트 원본 영상 내보내기\n",
    "filename = './output.mp4'\n",
    "fourcc = cv2.VideoWriter_fourcc(*'DIVX')\n",
    "fps = 3\n",
    "frameSize = (640, 640)\n",
    "isColor = True\n",
    "out = cv2.VideoWriter(filename, fourcc, fps, frameSize, isColor)\n",
    "for out_img in out_img_list:\n",
    "    out.write(out_img)\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bb8a5c-4387-4f66-ac14-c4fe9e954da5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo5test",
   "language": "python",
   "name": "yolo5test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
